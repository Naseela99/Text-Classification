


# -*- coding: utf-8 -*-
"""HW1_CSCI544-Naseela-Pervez.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BzZxD4MK26nyOmieA6kZLtXPTAlj2IwA
"""

# Python Version : 3.8

import pandas as pd
import numpy as np
import nltk
import re
from bs4 import BeautifulSoup

"""## Read Data"""

data = pd.read_table('data.tsv', on_bad_lines = 'skip',verbose = False)

"""## Keep Reviews and Ratings"""

new_data = data[['review_body','star_rating']]

""" ## We form three classes and select 20000 reviews randomly from each class.


"""

new_data = new_data.dropna()

def get_new_rating(rating):
  if int(rating) == 1 or int(rating) == 2:
    return 1
  elif int(rating) == 3:
    return 2
  else:
    return 3

new_data['class'] = new_data["star_rating"].apply(get_new_rating)

random_sample_class1 = new_data[new_data['class']==1].sample(n=20000,replace=False, random_state=42)
random_sample_class2 = new_data[new_data['class']==2].sample(n=20000,replace=False, random_state=42)
random_sample_class3 = new_data[new_data['class']==3].sample(n=20000,replace=False, random_state=42)

class_samples = [random_sample_class1,random_sample_class2,random_sample_class3]

df_class = pd.concat(class_samples).reset_index(drop=True)

"""# Data Cleaning


"""

before_cleaning = df_class['review_body'].str.len().mean()

# Convert all reviews to lowercase
df_class['review_body'] = df_class['review_body'].str.lower()

# Remove URLs
import re
url_pattern = r"http[s]\S+"
df_class['review_body'] = df_class['review_body'].replace(to_replace = url_pattern, value = '',regex=True)

# Remove HTML tags
html_pattern = r"<[^<]*>"
df_class['review_body'] = df_class['review_body'].replace(to_replace = html_pattern, value = '',regex=True)

# Perform contraction

import contractions

df_class['review_body'] = df_class['review_body'].apply(contractions.fix)

# Remove non-alphabetic characters
apha_str = r"[^a-z ]+"
df_class['review_body'] = df_class['review_body'].replace(to_replace = apha_str, value = '',regex=True,)

# Remove Extra Spaces
df_class['review_body'] = df_class['review_body'].apply(lambda x: ' '.join(x.split()))

after_cleaning = df_class['review_body'].str.len().mean()

print(before_cleaning,',',after_cleaning)

"""# Pre-processing

## remove the stop words
"""

before_preprocessing = df_class['review_body'].str.len().mean()


from nltk.corpus import stopwords

stopwords_eng = stopwords.words('english')

# stopwords_all = stopwords_eng + extra_words

ignore_stopwords = ["no", "not",'nor','very']
for kw in ignore_stopwords:
  stopwords_eng.remove(kw)

df_class['review_body'] = df_class['review_body'].apply(lambda x: ' '.join([w for w in x.split() if w not in stopwords_eng]))

##strip again
df_class['review_body'] = df_class['review_body'].apply(lambda x:x.strip())

## remove empty reviews
df_class = df_class[df_class['review_body']!='']

"""## perform lemmatization  """

# pos tagging

from nltk.corpus import wordnet
from nltk import pos_tag

def mapper(tags):

  if tags.startswith('N'):
    return wordnet.NOUN
  elif tags.startswith('VB'):
    return wordnet.VERB
  elif tags.startswith('JJ'):
    return wordnet.ADJ
  elif tags.startswith('RB'):
    return wordnet.ADV
  else:
    return None


from nltk.stem import WordNetLemmatizer

df_class[df_class["review_body"].apply(lambda x:len(x.strip()))==0]

word_lemmatizer = WordNetLemmatizer()

def lemmatization(text):


  words = text.split()
  # print(text, words)
  tags_out =  pos_tag(words)
  words, tags = list(zip(*tags_out))

  return " ".join([word_lemmatizer.lemmatize(word, pos=mapper(tag)) if mapper(tag) is not None else word for word, tag in zip(words, tags)])

df_class['review_body'] = df_class['review_body'].apply(lemmatization)

after_preprocessing =  df_class['review_body'].str.len().mean()

print(before_preprocessing, ',', after_preprocessing)

"""# TF-IDF Feature Extraction"""

# Train test split

from sklearn.model_selection import train_test_split

# X = data_df.drop(['class'],axis=1)
X = df_class['review_body']

# y = data_df['class']
y = df_class['class']

X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2)

from sklearn.feature_extraction.text import TfidfVectorizer

tfidf = TfidfVectorizer(max_features = 1000,ngram_range=(1,5))

features_train = tfidf.fit_transform(X_train).toarray()

features_test = tfidf.transform(X_test).toarray()

X_train = pd.DataFrame(data = features_train)
X_test = pd.DataFrame(data = features_test)

"""# Perceptron"""

from sklearn.linear_model import Perceptron

# perceptron_classifier = Perceptron(alpha=0.8, max_iter=10, verbose=1, eta0=1, tol=None)
perceptron_classifier = Perceptron()
perceptron_classifier.fit(X_train,y_train)

preds = perceptron_classifier.predict(X_test)

from sklearn.metrics import classification_report, precision_score 


rep = classification_report(y_test, preds,output_dict=True)

print("\n",rep['1']['precision'],',',rep['1']['recall'],',',rep['1']['f1-score'])
print("\n",rep['2']['precision'],',',rep['2']['recall'],',',rep['2']['f1-score'])
print("\n",rep['3']['precision'],',',rep['3']['recall'],',',rep['3']['f1-score'])
print("\n",rep['macro avg']['precision'],',',rep['macro avg']['recall'],',',rep['macro avg']['f1-score'])

"""# SVM"""

from sklearn.svm import LinearSVC
svc = LinearSVC()
svc.fit(X_train,y_train)

preds = svc.predict(X_test)


rep = classification_report(y_test, preds,output_dict=True)

print("\n",rep['1']['precision'],',',rep['1']['recall'],',',rep['1']['f1-score'])
print("\n",rep['2']['precision'],',',rep['2']['recall'],',',rep['2']['f1-score'])
print("\n",rep['3']['precision'],',',rep['3']['recall'],',',rep['3']['f1-score'])
print("\n",rep['macro avg']['precision'],',',rep['macro avg']['recall'],',',rep['macro avg']['f1-score'])

"""# Logistic Regression"""

from sklearn.linear_model import LogisticRegression

logreg_classifier = LogisticRegression(max_iter = 1000)

logreg_classifier.fit(X_train,y_train)

preds = logreg_classifier.predict(X_test)


rep = classification_report(y_test, preds,output_dict=True)

print("\n",rep['1']['precision'],',',rep['1']['recall'],',',rep['1']['f1-score'])
print("\n",rep['2']['precision'],',',rep['2']['recall'],',',rep['2']['f1-score'])
print("\n",rep['3']['precision'],',',rep['3']['recall'],',',rep['3']['f1-score'])
print("\n",rep['macro avg']['precision'],',',rep['macro avg']['recall'],',',rep['macro avg']['f1-score'])

"""# Naive Bayes"""

from sklearn.naive_bayes import MultinomialNB

gnb = MultinomialNB()

gnb.fit(X_train,y_train)

preds = gnb.predict(X_test)


rep = classification_report(y_test, preds,output_dict=True)

print("\n",rep['1']['precision'],',',rep['1']['recall'],',',rep['1']['f1-score'])
print("\n",rep['2']['precision'],',',rep['2']['recall'],',',rep['2']['f1-score'])
print("\n",rep['3']['precision'],',',rep['3']['recall'],',',rep['3']['f1-score'])
print("\n",rep['macro avg']['precision'],',',rep['macro avg']['recall'],',',rep['macro avg']['f1-score'])
